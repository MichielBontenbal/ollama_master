{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama advanced exercises\n",
    "\n",
    "**Running LLM's locally - Master Applied AI - Michiel Bontenbal - 12 december 2024**\n",
    "\n",
    "Ollama is a tool that allows users to run open-source large language models (LLMs) locally on your laptop. Ollama supports a variety of models, including Llama2, Mistral, CodeLlama and many others. \n",
    "\n",
    "You'll need to download ollama first. Download it from www.ollama.com.\n",
    "You'll also need to do the notebook 'ollama.ipynb' first to get a basic understanding of ollama.\n",
    "\n",
    "\n",
    "### Contents\n",
    "0. Installs, checks and imports\n",
    "1. Download GEITje from Huggingface & run it\n",
    "2. Download FIETje from Huggingface & run it\n",
    "3. Compare two models in a Gradio frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installs, checks and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your version of python. To run ollama with python you will need Python 3.8 or higher.\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you run from harddisk. Running this from OneDrive makes it much slower.\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and run GEITje from Huggingface\n",
    "\n",
    "#### Exercise: 1 Download a model from the Huggingface Hub\n",
    "\n",
    "Steps:\n",
    "1. Go to Huggingface Hub\n",
    "2. Find the model 'GEITje-7B-ultra-GGUF'\n",
    "3. Check the different versions that are available!\n",
    "4. Check also under 'files' the different versions & their size.\n",
    "4. Click the button 'Use this model' to use it with Ollama.\n",
    "5. Pick one version that is suitable for your laptop! (small/fast enough to run)\n",
    "6. Make sure you'll change it to ollama pull <modelname>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull hf.co/BramVanroy/GEITje-7B-ultra-GGUF:Q3_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Run this model using python\n",
    "\n",
    "Copy paste code from the notebook 'ollama.ipynb' to do this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the models on your device\n",
    "model_list = []\n",
    "models = ollama.list()\n",
    "modellen = models.models\n",
    "for i in range (len(modellen)):\n",
    "    print(models.models[i].model)\n",
    "    model_list.append(models.models[i].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the model. copy paste.\n",
    "model = 'hf.co/BramVanroy/fietje-2-chat-gguf:Q3_K_M'\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR SOLUTION HERE\n",
    "import ollama\n",
    "\n",
    "def ask_ollama(question, model):\n",
    "    \"\"\"\n",
    "    \n",
    "    Sends a question to the Ollama API and returns the response.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "ask_ollama('vertel een mop', model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and run FIETje\n",
    "\n",
    "Find Fietje at https://huggingface.co/BramVanroy/fietje-2-chat-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama run hf.co/BramVanroy/fietje-2-chat-gguf:Q3_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare two models\n",
    "We can compare two models side by side by running the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "models = model_list #TO DO Change models!\n",
    "\n",
    "def compare_models(prompt, model1, model2):\n",
    "    response1 = ask_ollama(prompt, model1)\n",
    "    response2 = ask_ollama(prompt, model2)\n",
    "    return response1, response2\n",
    "\n",
    "  # Add or modify this list based on available models\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=compare_models,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your prompt\"),\n",
    "        gr.Dropdown(choices=models, label=\"Select Model 1\"),\n",
    "        gr.Dropdown(choices=models, label=\"Select Model 2\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Model 1 Response\"),\n",
    "        gr.Textbox(label=\"Model 2 Response\")\n",
    "    ],\n",
    "    title=\"LLM Model Comparison Arena\",\n",
    "    description=\"Compare responses from two different LLM models side by side.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
