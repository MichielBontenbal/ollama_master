{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c409dcaf",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "**Running LLM's locally - Master Applied AI - Michiel Bontenbal - 12 december 2024**\n",
    "\n",
    "Ollama is a tool that allows users to run open-source large language models (LLMs) locally on your laptop. Ollama supports a variety of models, including Llama2, Mistral, CodeLlama and many others. \n",
    "\n",
    "You'll need to download ollama first. Download it from www.ollama.com.\n",
    "\n",
    "Courtesy of some code examples to ollama.com / Jeffrey Morgan.\n",
    "License: MIT License\n",
    "\n",
    "### Contents\n",
    "0. Install and settings\n",
    "1. First script\n",
    "2. Streaming the response\n",
    "3. Create a gradio front end\n",
    "\n",
    "### Sources\n",
    "- https://github.com/ollama/ollama-python\n",
    "- https://github.com/ollama/ollama/blob/main/docs/api.md#api\n",
    "- https://pypi.org/project/ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669c8d4",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Install and settings\n",
    "\n",
    "*Before running this code, make sure you've installed ollama on your laptop!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbaed4",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Check your version of python. To run ollama with python you will need Python 3.8 or higher.\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfdd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before downloading the model check available disk space. You will need at least 20 Gb!\n",
    "import shutil\n",
    "usage = shutil.disk_usage(\"/\")\n",
    "free_space_bytes = usage.free\n",
    "free_space_gb = free_space_bytes / (1024 * 1024 * 1024)  # Convert to GB\n",
    "print(f'free disk space = {round(free_space_gb,1)} Gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aae086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check processor and RAM\n",
    "import psutil\n",
    "import platform\n",
    "print(\"Processor:\", platform.processor())\n",
    "memory = psutil.virtual_memory()\n",
    "print(f'Total RAM: \"{memory.total/1000000000} Gb')\n",
    "print(f\"Available RAM: {memory.available/1000000000} Gb\")\n",
    "print(f\"RAM Usage: {memory.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd224ea",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf28a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you run from harddisk. Running this from OneDrive or cloud makes it much slower.\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215465c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#download a model from the ollama server. May take a minute... Uncomment if necessary\n",
    "import ollama\n",
    "ollama.pull('llama3.2:1b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the models on your device\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01decc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's unpack it a bit (ollama changed it's API this week...) so \n",
    "models = ollama.list()\n",
    "print(models)\n",
    "modellen = models.models\n",
    "for i in range (len(modellen)):\n",
    "    print(models.models[i].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446fb78",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#printing the details of a model\n",
    "ollama.show('llama3.2:1b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76635e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#show all functions\n",
    "print(dir(ollama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9609c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Delete a model. \n",
    "#ollama.delete(<your model>) #replace <your model>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1ae15",
   "metadata": {},
   "source": [
    "## 1. Run first script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first set the model\n",
    "model = 'llama3.2:1b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ccf41",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#first script from ollama website (https://github.com/ollama/ollama-python)\n",
    "import ollama\n",
    "response = ollama.chat(model=model, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af2af5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Create the ollama function\n",
    "import ollama\n",
    "\n",
    "def ask_ollama(question, model):\n",
    "    \"\"\"\n",
    "    \n",
    "    Sends a question to the Ollama API and returns the response.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example usage\n",
    "response_content = ask_ollama(\"Why is the sky blue?\", model)\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e149d",
   "metadata": {},
   "source": [
    "## 2. Streaming the response\n",
    "\n",
    "With streaming the response will be printed on the screen while the LLM is still busy generating the answer. This is a faster solution. Try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377debd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "question = input('Your question:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e898ef",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#same but now as a function (to use with gradio) \n",
    "import ollama\n",
    "\n",
    "def ollama_chat_stream(question):\n",
    "    \"\"\"\n",
    "    Streams the chat response from Ollama using the 'tinyllama' model.\n",
    "    \"\"\"\n",
    "    # Initialize the chat with Ollama\n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': question}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    # Stream and print the responses\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        #print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "# Example usage\n",
    "ollama_chat_stream(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a70748",
   "metadata": {},
   "source": [
    "## 3. Creating a gradio front end\n",
    "\n",
    "Gradio is a very high level Python library that let's you create a front-end very quickly. It is used to demo your model. Gradio starts a server for you (like Flask or NodeJS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f33c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment if necessary\n",
    "!pip install gradio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f550b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a11538",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#a Gradio frontend make sure you have run previous cells\n",
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=ask_ollama,  #use the function we defined under 1\n",
    "    inputs=\"text\", \n",
    "    outputs= \"text\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
