{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Dutch Models\n",
    "\n",
    "**Running LLM's locally - Master Applied AI - Michiel Bontenbal - 11 december 2025**\n",
    "\n",
    "Ollama is a tool that allows users to run open-source large language models (LLMs) locally on your laptop. Ollama supports a variety of models, including Llama2, Mistral, CodeLlama and many others. \n",
    "\n",
    "You'll need to download ollama first. Download it from www.ollama.com.\n",
    "You'll also need to do the notebook 'ollama.ipynb' first to get a basic understanding of ollama.\n",
    "\n",
    "\n",
    "### Contents\n",
    "0. Installs, checks and imports\n",
    "1. Download Schaapje from Huggingface & run it\n",
    "2. Download FIETje from Huggingface & run it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installs, checks and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your version of python. To run ollama with python you will need Python 3.8 or higher.\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you run from harddisk. Running this from OneDrive makes it much slower.\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of all models available on your machine\n",
    "import ollama\n",
    "\n",
    "models = ollama.list()\n",
    "#print(models)\n",
    "modellen = models.models\n",
    "for i in range (len(modellen)):\n",
    "    print(models.models[i].model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and run Schaapje from Huggingface\n",
    "\n",
    "#### Exercise: 1 Download a model from the Huggingface Hub\n",
    "\n",
    "Steps:\n",
    "1. Go to Huggingface Hub\n",
    "2. Find the model 'Schaapje-2B-Chat-V1.0-GGUF'\n",
    "3. Check the different versions that are available!\n",
    "4. Check also under 'files' the different versions & their size.\n",
    "5. Click the button 'Use this model' to use it with Ollama.\n",
    "6. Pick one version that is suitable for your laptop! (small/fast enough to run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Run this model using python\n",
    "\n",
    "Copy paste code from the notebook 'ollama.ipynb' to do this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the models on your device\n",
    "model_list = []\n",
    "models = ollama.list()\n",
    "modellen = models.models\n",
    "for i in range (len(modellen)):\n",
    "    print(models.models[i].model)\n",
    "    model_list.append(models.models[i].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the model. copy paste.\n",
    "model = ' '\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and run FIETje\n",
    "\n",
    "Find Fietje at https://huggingface.co/BramVanroy/fietje-2-chat-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR SOLUTION TO PULL THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now use the streamlit app to compare the two models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
