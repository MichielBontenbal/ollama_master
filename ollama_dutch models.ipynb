{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama run Dutch Models\n",
    "\n",
    "**Running LLM's locally - Master Applied AI - Michiel Bontenbal - 12 december 2024**\n",
    "\n",
    "Ollama is a tool that allows users to run open-source large language models (LLMs) locally on your laptop. Ollama supports a variety of models, including Llama2, Mistral, CodeLlama and many others. \n",
    "\n",
    "You'll need to download ollama first. Download it from www.ollama.com.\n",
    "You'll also need to do the notebook 'ollama.ipynb' first to get a basic understanding of ollama.\n",
    "\n",
    "\n",
    "### Contents\n",
    "0. Installs, checks and imports\n",
    "1. Download GEITje from Huggingface & run it\n",
    "2. Download FIETje from Huggingface & run it\n",
    "3. Compare two models in a Gradio frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installs, checks and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.7\n"
     ]
    }
   ],
   "source": [
    "# Check your version of python. To run ollama with python you will need Python 3.8 or higher.\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/michielbontenbal/Library/CloudStorage/OneDrive-HvA/GitHub/ollama_master\n"
     ]
    }
   ],
   "source": [
    "# Make sure you run from harddisk. Running this from OneDrive makes it much slower.\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3.2:1b', modified_at=datetime.datetime(2024, 12, 10, 18, 27, 0, 102006, tzinfo=TzInfo(+01:00)), digest='baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878', size=1321098329, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.2B', quantization_level='Q8_0')), Model(model='hf.co/BramVanroy/fietje-2-chat-gguf:Q3_K_M', modified_at=datetime.datetime(2024, 12, 10, 13, 37, 12, 900267, tzinfo=TzInfo(+01:00)), digest='29b0a169fcaa64dca25ab1b5325a26f8dad0e42217460ca1694fac629c902035', size=1423223271, details=ModelDetails(parent_model='', format='gguf', family='phi2', families=['phi2'], parameter_size='2.78B', quantization_level='unknown')), Model(model='hf.co/BramVanroy/GEITje-7B-ultra-GGUF:Q3_K_M', modified_at=datetime.datetime(2024, 12, 10, 13, 10, 1, 192779, tzinfo=TzInfo(+01:00)), digest='7595df917f18a22cc1ee275332b7ebb8b23e8976a542e5dd6c74c1c8ac3d6304', size=3518986848, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.24B', quantization_level='unknown')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2024, 12, 10, 11, 32, 51, 605312, tzinfo=TzInfo(+01:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='moondream:1.8b', modified_at=datetime.datetime(2024, 11, 3, 15, 37, 23, 887047, tzinfo=TzInfo(+01:00)), digest='55fc3abd386771e5b5d1bbcc732f3c3f4df6e9f9f08f1131f9cc27ba2d1eec5b', size=1738451197, details=ModelDetails(parent_model='', format='gguf', family='phi2', families=['phi2', 'clip'], parameter_size='1B', quantization_level='Q4_0')), Model(model='nomic-embed-text:latest', modified_at=datetime.datetime(2024, 5, 15, 12, 10, 55, 317501, tzinfo=TzInfo(+02:00)), digest='0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f', size=274302450, details=ModelDetails(parent_model='', format='gguf', family='nomic-bert', families=['nomic-bert'], parameter_size='137M', quantization_level='F16'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and run GEITje from Huggingface\n",
    "\n",
    "#### Exercise: 1 Download a model from the Huggingface Hub\n",
    "\n",
    "Steps:\n",
    "1. Go to Huggingface Hub\n",
    "2. Find the model 'GEITje-7B-ultra-GGUF'\n",
    "3. Check the different versions that are available!\n",
    "4. Check also under 'files' the different versions & their size.\n",
    "4. Click the button 'Use this model' to use it with Ollama.\n",
    "5. Pick one version that is suitable for your laptop! (small/fast enough to run)\n",
    "6. Make sure you'll change it to ollama pull <modelname>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Run this model using python\n",
    "\n",
    "Copy paste code from the notebook 'ollama.ipynb' to do this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.2:1b\n",
      "hf.co/BramVanroy/fietje-2-chat-gguf:Q3_K_M\n",
      "hf.co/BramVanroy/GEITje-7B-ultra-GGUF:Q3_K_M\n",
      "llama3.2:latest\n",
      "moondream:1.8b\n",
      "nomic-embed-text:latest\n"
     ]
    }
   ],
   "source": [
    "#List all the models on your device\n",
    "model_list = []\n",
    "models = ollama.list()\n",
    "modellen = models.models\n",
    "for i in range (len(modellen)):\n",
    "    print(models.models[i].model)\n",
    "    model_list.append(models.models[i].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select the model. copy paste.\n",
    "model = ' '\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and run FIETje\n",
    "\n",
    "Find Fietje at https://huggingface.co/BramVanroy/fietje-2-chat-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR SOLUTION TO PULL THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare two models\n",
    "We can compare two models side by side by running the code below. \n",
    "\n",
    "Make sure you have updated the model_list to also get FIETje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the code\n",
    "import gradio as gr\n",
    "\n",
    "models = model_list \n",
    "\n",
    "def compare_models(prompt, model1, model2):\n",
    "    response1 = ask_ollama(prompt, model1)\n",
    "    response2 = ask_ollama(prompt, model2)\n",
    "    return response1, response2\n",
    "\n",
    "  # Add or modify this list based on available models\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=compare_models,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your prompt\"),\n",
    "        gr.Dropdown(choices=models, label=\"Select Model 1\"),\n",
    "        gr.Dropdown(choices=models, label=\"Select Model 2\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Model 1 Response\"),\n",
    "        gr.Textbox(label=\"Model 2 Response\")\n",
    "    ],\n",
    "    title=\"LLM Model Comparison Arena\",\n",
    "    description=\"Compare responses from two different LLM models side by side.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
